{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with HTRC Extracted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will get you up-and-running with the HTRC Extracted Features dataset. Learn more about the data: https://wiki.htrc.illinois.edu/x/GoA5Ag\n",
    "\n",
    "The code and instructions used in this notebook combine elements from a Programming Historian lesson called\"Text Mining in Python through the HTRC Feature Reader\" (https://programminghistorian.org/en/lessons/text-mining-with-extracted-features) and the Berkeley Data Science Module, \"Library-HTRC\" (https://github.com/ds-modules/Library-HTRC).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up and reading in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we need to import the Python modules we'll use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy \n",
    "import pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted Featuers files are originally formatted in JSON notation and compressed; you'll notice the file format is '.json.bz2'. The FeatureReader library is able to work with the files in that format with needing to decompress the files.\n",
    "\n",
    "Within the library, there is a **FeatureReader object** that is used for loading the dataset files and making sense of them. It returns a **Volume object** for each file. A Volume is a representation of a single item in HathiTrust, for example a book or other textual work. From the Volume, you can access features about the work. To drill down to the features derived from individual pages, use the **Page object**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to get the FeatureReader ready to use by pointing it to the file paths for the sample Extracted Features files we are using in this notebook. The files are in directory called 'data', in which they are further divided into two directories: '1970' and '1930'. \n",
    " \n",
    "With fr = FeatureReader(paths) below, the FeatureReader is initialized, meaning it is ready to use. An initialized FeatureReader is holding references to the file paths that we gave it, and will load them into Volume objects when asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a list of file names from the 1970s directory, create a list of paths to those files, and load\n",
    "#the data in the Feature Reader\n",
    "\n",
    "filenames1970 = ['mdp.49015002203033.json.bz2', 'mdp.49015002203405.json.bz2', 'mdp.49015002203140.json.bz2', \n",
    "'mdp.49015002221761.json.bz2', 'mdp.49015002203157.json.bz2', 'mdp.49015002221779.json.bz2', 'mdp.49015002203215.json.bz2', \n",
    "'mdp.49015002221787.json.bz2', 'mdp.49015002203223.json.bz2', 'mdp.49015002221811.json.bz2', 'mdp.49015002203231.json.bz2', \n",
    "'mdp.49015002221829.json.bz2', 'mdp.49015002203249.json.bz2', 'mdp.49015002221837.json.bz2', 'mdp.49015002203272.json.bz2', \n",
    "'mdp.49015002221845.json.bz2']\n",
    "file_paths_1970 = ['data/1970/' + file for file in filenames1970]\n",
    "fr1970 = FeatureReader(file_paths_1970)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what titles have been loaded as Volumes. Because these are serials, they have the same basic title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vol in fr1970.volumes():\n",
    "    print(vol.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bonus\n",
    "Remove the comments from the following code box and re-run the following examples using the 1930s dataset. Do you find anything interesting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filenames1930 = ['mdp.49015002221860.json.bz2', 'miua.4925052,1928,001.json.bz2', 'mdp.49015002221878.json.bz2',\n",
    "#'miua.4925383,1934,001.json.bz2', 'mdp.49015002221886.json.bz2']\n",
    "#file_paths_1930 = ['data/1970/' + file for file in filenames]\n",
    "#fr1930 = FeatureReader(file_paths_1930)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File and page structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call just one Volume at a time in order to examine its contents. In this example, we are taking the first file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = fr1970.first()\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call the URL for the volume and CTRL-click it to find the corresponding item in the HathiTrust Digital Library (HTDL). \n",
    "\n",
    "These volumes are in the public domain, so we will find that they are available for \"Full View\" in the HTDL. If they were still under copyright, we would be taken to a \"Limited View\" page. The Extracted Features dataset includes a snapshot of 15.7 million volumes from the HTDL and is agnostic to rights status, as the files represent data about the volumes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vol.handle_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what other metadata elements are available to you for each item in its corresponding Extracted Features file. Put your cursor between the period and the end parenthesis, and press tab. You can choose from the dropdown list. Then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your cursor between the period . and the end parenthesis ) and press tab. You can choose from the dropdown list.\n",
    "print(vol.author)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to access the first features of vol, which is a table of total words for every single page. These can be accessed simply by calling vol.tokens_per_page()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol.tokens_per_page()\n",
    "# Show just the first few rows, so we can look at what it looks like\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a straightforward table of information, similar to what you would see in Excel or Google Spreadsheets. Listed in the table are page numbers and the count of words on each page. \n",
    "\n",
    "With only two dimensions, it is trivial to plot the number of words per page. The table structure holding the data has a plot method for data graphics. Without extra arguments, tokens.plot() will assume that you want a line chart with the page on the x-axis and word count on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tokens.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we get here? When we ran vol.tokens_per_page(), it returned a Pandas DataFrame. This means that after setting tokens, we're no longer working with HTRC-specific code, just book data held in a common and very robust table-like construct from Pandas. tokens.head() used a DataFrame method to look at the first few rows of the dataset, and tokens.plot() uses a method from Pandas to visualize data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another DataFrame accessible to us is vol.tokenlist(), which can be called to return section-, part-of-speech-, and word-specific details:\n",
    "\n",
    "Now let's look at some words deeper into the book: from 1000th to 1100th row, skipping by 15 [1000:1100:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = vol.tokenlist()\n",
    "tl[1000:1100:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas DataFrame type returned by the HTRC Feature Reader is very malleable. To work with the tokenlist that you retrieved earlier, three skills are particularily valuable: selecting subsets by a condition, slicing by named row index, and grouping and aggregating. \n",
    "\n",
    "We'll practice slicing by named row index first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can add a word between the quotation marks to retrieve only pages where that word occurs. We are using the power of the DataFrame index to retrieve only the rows that match our critia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a word between the quotes\n",
    "tl_all = vol.tokenlist(section='all')\n",
    "chapter_pages = tl_all.loc[(slice(None), slice(None), \"\"),]\n",
    "chapter_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will subset the data by selection. \n",
    "\n",
    "To subset individual rows of a DataFrame, provide a series of True/False values to the DataFrame, formatted in square brackets. When True, the DataFrame returns that row; when False, the row is excluded from what is returned. \n",
    "\n",
    "t1_simple returns a token list without part-of-speech or page information. Then, we set matches to find only those rows that are \"True\" for the selection criteria of occuring more than 100 times. Finally, we sample 5 rows in our subset. Try adjusting the numbers in the code below to change the number of rows sampled or the counts on which you match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#adjust the matching count number or sample set\n",
    "tl_simple = vol.tokenlist(pos=False, pages=False)\n",
    "matches = tl_simple['count'] > 100\n",
    "tl_simple[matches].sample(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still looking at one volume, let's start to explore the relative frequencies of tokens within the volume. \n",
    "\n",
    "The following cell will display the most common tokens (words or punctuation marks) in a given volume, alongside the number of times they appear. It will also calculate their relative frequencies (found by dividing the number of appearances over the total number of words in the book) and display the results in a DataFrame. The cell may take a few seconds to run because we're looping through every word in the volume!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol.tokenlist(pos=False, case=False, pages=False).sort_values('count', ascending=False)\n",
    "\n",
    "freqs = []\n",
    "for count in tokens['count']:\n",
    "    freqs.append(count/sum(tokens['count']))\n",
    "    \n",
    "tokens['rel_frequency'] = freqs\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the most common tokens from the volume and their frequencies. The following cell outputs a bar plot using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Build a list of frequencies and a list of tokens.\n",
    "freqs_1, tokens_1 = [], []\n",
    "for i in range(15):  # top 8 words\n",
    "    freqs_1.append(freqs[i])\n",
    "    tokens_1.append(tokens.index.get_level_values('lowercase')[i])\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = numpy.arange(len(tokens_1))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_1)\n",
    "plt.xticks(x_ticks, tokens_1, rotation=90)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:14] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most common tokens are mostly punctuation and basic words that don't provide context. Let's see if we can narrow our search to gain some more relevant insight. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get a list of stopwords from the nltk library. Punctuation is in the string library. Let's import nltk and make the stopwords and punctuation accessible to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "print()\n",
    "\n",
    "print(punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of words to ignore in our DataFrame, we can make a few tweaks to our plotting cell to remove the punctuation and display only those words not in our stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_filtered, tokens_filtered, i = [], [], 0\n",
    "while len(tokens_filtered) < 10:\n",
    "    if tokens.index.get_level_values('lowercase')[i] not in stopwords.words('english') + list(punctuation):\n",
    "        freqs_filtered.append(freqs[i])\n",
    "        tokens_filtered.append(tokens.index.get_level_values('lowercase')[i])\n",
    "    i += 1\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = numpy.arange(len(freqs_filtered))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_filtered)\n",
    "plt.xticks(x_ticks, tokens_filtered, rotation=90)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:14] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens from all volumes in our set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how word frequencies compare across all the books in our samples. \n",
    "\n",
    "First we'll set-up a few functions. The first finds the most common noun in a volume, with adjustable parameters for minimum length. The second calculates the relative frequency of a token across the entirety of a volume, saving us the time of doing the calculation like in the above cell. Finally, we'll have a visualization function to create a bar plot of relative frequencies for all volumes in our sample, so that we can easily track how word frequencies differ across titles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "Let's see what the most common nouns in this work are by word length. To try, add a number to the second code box below.\n",
    "\n",
    "NOTE: word_length defaults to 2. e.g. most_common_noun(fr_novels.first) returns 'time'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establishing the function used in the next box\n",
    "def most_common_noun(vol, word_length=2):   \n",
    "    # Build a table of common nouns\n",
    "    tokens_1 = vol.tokenlist(pages=False, case=False)\n",
    "    nouns_only = tokens_1.loc[(slice(None), slice(None), ['NN']),]\n",
    "    top_nouns = nouns_only.sort_values('count', ascending=False)\n",
    "\n",
    "    token_index = top_nouns.index.get_level_values('lowercase')\n",
    "    \n",
    "    # Choose the first token at least as long as word_length with non-alphabetical characters\n",
    "    for i in range(max(token_index.shape)):\n",
    "        if (len(token_index[i]) >= word_length):\n",
    "            if(\"'\", \"!\", \",\", \"?\" not in token_index[i]):\n",
    "                return token_index[i]\n",
    "    print('There is no noun of this length')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a number to the parenthesis between the comma , and end parenthesis )\n",
    "most_common_noun(vol, )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "Here, the function frequency() returns a plot of the usage frequencies of the given word across all volumes in the given FeatureReader collection.\n",
    "\n",
    "NOTE: frequency() returns a dictionary entry of the form {'word': frequency}. e.g. frequency(fr_novels.first(), 'blue') returns {'blue': 0.00012}\n",
    "\n",
    "Try adding a word in the single quotes in the last line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establishing the function used in the next box\n",
    "def frequency(vol, word):\n",
    "    t1 = vol.tokenlist(pages=False, pos=False, case=False)\n",
    "    token_index = t1[t1.index.get_level_values(\"lowercase\") == word]\n",
    "    \n",
    "    if len(token_index['count'])==0:\n",
    "        return {word: 0}\n",
    "    \n",
    "    count = token_index['count'][0]\n",
    "    freq = count/sum(t1['count'])\n",
    "    \n",
    "    return {word: float('%.5f' % freq)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a word in the quotes below\n",
    "frequency(vol, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together\n",
    "\n",
    "The code below returns a plot of the usage frequencies of the given word across all volumes in the given FeatureReader collection.\n",
    "\n",
    "Try adding different words to see their relative frequency in our sample.\n",
    "\n",
    "NOTE: frequencies are given as percentages rather than true ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establishing the function used in the next box\n",
    "def frequency_bar_plot(word, fr):\n",
    "    freqs, titles = [], []\n",
    "    for vol in fr:\n",
    "        title = vol.title\n",
    "        short_title = title[:6] + (title[6:] and '..')\n",
    "        freqs.append(100*frequency(vol, word)[word])\n",
    "        titles.append(short_title)\n",
    "        \n",
    "    # Format and plot the data\n",
    "    x_ticks = numpy.arange(len(titles))\n",
    "    plt.bar(x_ticks, freqs)\n",
    "    plt.xticks(x_ticks, titles, fontsize=10, rotation=45)\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.title('Frequency of \"' + word + '\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a word to between the quotes\n",
    "frequency_bar_plot('', fr_novels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's interesting, but since all our titles are the same, it's hard to make sense of the results. Let's try plotting relative frequency over time.\n",
    "\n",
    "The code below returns a DataFrame of relative frequencies, volume years, and page counts, along with a scatter plot.\n",
    "\n",
    "NOTE: frequencies are given in percentages rather than true ratios.\n",
    "\n",
    "Try adding a word in the single quotes in the last line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#establishing the function used in the next box\n",
    "def frequency_by_year(query_word, fr):\n",
    "    volumes = pandas.DataFrame()\n",
    "    years, page_counts, query_freqs = [], [], []\n",
    "\n",
    "    for vol in fr:\n",
    "        years.append(int(vol.year))\n",
    "        page_counts.append(int(vol.page_count))\n",
    "        query_freqs.append(100*frequency(vol, query_word)[query_word])\n",
    "    \n",
    "    volumes['year'], volumes['pages'], volumes['freq'] = years, page_counts, query_freqs\n",
    "    volumes = volumes.sort_values('year')\n",
    "    \n",
    "    # Set plot dimensions and labels\n",
    "    scatter_plot = volumes.plot.scatter('year', 'freq', color='black', s=50, fontsize=12)\n",
    "    plt.ylim(0-numpy.mean(query_freqs), max(query_freqs)+numpy.mean(query_freqs))\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.title('Frequency of \"' + query_word + '\"', fontsize=14)\n",
    "    \n",
    "    return volumes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a word to between the quotes\n",
    "frequency_by_year('', fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making use of the structured file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particularly useful thing about the Extracted Features dataset is that the tokens in the extracted features files are part-of-speech tagged to differentiate homynyms like rose, which can be a name, a noun, and a verb.\n",
    "\n",
    "For each page, the data is also divided into a header, body, and footer section so that you can systematically remove headers or footers from your data if you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw the possibiity of drilling down to the part-of-speech tag earlier when we found the most frequently-occuring noun in a volume. Below, we will look for one part of speech in just the body of our volumes.\n",
    "\n",
    "What do you find? Try editing the code to retrieve tokens of another part of speech. Here are the codes in the Penn Treebank: https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pandas.IndexSlice\n",
    "vol = next(fr.volumes())\n",
    "tl = vol.tokenlist(pages=False)\n",
    "tl.index = tl.index.droplevel(0)\n",
    "adjectives = tl.loc[idx[:,('JJ')],]\n",
    "adj_dfs = [adjectives for vol in fr.volumes()]\n",
    "all_adj = pandas.concat(adj_dfs).groupby(level='token').sum().sort_values('count', ascending=False)[:50]\n",
    "\n",
    "#prints the Pandas dataframe of all adjectives.\n",
    "print(all_adj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
