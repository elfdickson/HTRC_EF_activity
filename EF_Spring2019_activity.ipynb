{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting started with HTRC Extracted Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This tutorial will get you up-and-running with the HTRC Extracted Features dataset. Learn more about the data: https://wiki.htrc.illinois.edu/x/GoA5Ag\n",
    "\n",
    "The code and instructions used in this notebook combine elements from a Programming Historian lesson,\"Text Mining in Python through the HTRC Feature Reader\" (https://programminghistorian.org/en/lessons/text-mining-with-extracted-features) and the Berkeley Data Science Module, \"Library-HTRC\" (https://github.com/ds-modules/Library-HTRC).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set-up and reading in files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get started, we need to import modules we'll use throughout this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from htrc_features import FeatureReader\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy \n",
    "import pandas "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extracted Featuers files are originally formatted in JSON notation and compressed; you'll notice the file format is '.json.bz2'. The FeatureReader library is able to work with the files in that format. \n",
    "\n",
    "The FeatureReader object is the interface for loading the dataset files and making sense of them, and it returns a Volume object for each file. A Volume is a representation of a single iteam in the HathiTrust, for example a book or other textual work. From the Volume, access features about a work. To drill down to the features derived from individual pages, use the Page object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll need to get the FeatureReader ready to use by pointing it to the file path to the sample Extracted Features files we are using in this notebook. The files are in directory called 'data', in which they are further divided into two directories: '1970' and '1930'. We refer to these directories as \"decade\" in the code below, as they allow us to compare presidential speeches from the 1930s to the 1970s. \n",
    " \n",
    "With fr = FeatureReader(paths), the FeatureReader is initialized, meaning it is ready to use. An initialized FeatureReader is holding references to the file paths that we gave it, and will load them into Volume objects when asked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_folder = 'data/1970'\n",
    "suffix = '.json.bz2'\n",
    "file_paths = ['data/1970' + file for file in data_folder if path.endswith(suffix)]\n",
    "#paths = glob.glob('/data/1970/*.json.bz2')\n",
    "fr = FeatureReader(file_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what titles have been loaded as Volumes. Because these are serials, they have the same basic title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for vol in fr.volumes():\n",
    "    print(vol.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File and page structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can call just one file at a time in order to examine its contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vol = fr.first()\n",
    "vol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also call the URL for the volume and click it to find the corresponding item in the HathiTrust Digital Library (HTDL). These volumes are in the public domain, so we will find that they are available for \"Full View\" in the HTDL. If they were still under copyright, we would be taken to a \"Limited View\" page; the Extracted Featues dataset includes a snapshot of 15.7 million volumes from the HTDL and is agnostic to rights status as they represent data about the volume. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vol.handle_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what other metadata elements are available to you for each volume in its corresponding Extracted Features file. Put your cursor between the period and the end parenthesis, and press tab. You can choose from the dropdown list. Then run the cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put your cursor between the period . and the end parenthesis ) and press tab. You can choose from the dropdown list.\n",
    "print(vol.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's time to access the first features of vol: a table of total words for every single page. These can be accessed simply by calling vol.tokens_per_page()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol.tokens_per_page()\n",
    "# Show just the first few rows, so we can look at what it looks like\n",
    "tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a straightforward table of information, similar to what you would see in Excel or Google Spreadsheets. Listed in the table are page numbers and the count of words on each page. \n",
    "\n",
    "With only two dimensions, it is trivial to plot the number of words per page. The table structure holding the data has a plot method for data graphics. Without extra arguments, tokens.plot() will assume that you want a line chart with the page on the x-axis and word count on the y-axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "tokens.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did we get here? When we ran vol.tokens_per_page(), it returned a Pandas DataFrame. This means that after setting tokens, we're no longer working with HTRC-specific code, just book data held in a common and very robust table-like construct from Pandas. tokens.head() used a DataFrame method to look at the first few rows of the dataset, and tokens.plot() uses a method from Pandas to visualize data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another DataFrame accessible to us is vol.tokenlist(), which can be called to return section-, part-of-speech-, and word-specific details:\n",
    "\n",
    "Now let's look at some words deeper into the book: from 1000th to 1100th row, skipping by 15 [1000:1100:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl = vol.tokenlist()\n",
    "tl[1000:1100:15]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Pandas DataFrame type returned by the HTRC Feature Reader is very malleable. To work with the tokenlist that you retrieved earlier, three skills are particularily valuable:\n",
    "\n",
    "    Selecting subsets by a condition\n",
    "    Slicing by named row index\n",
    "    Grouping and aggregating\n",
    "\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADDDDDDD\n",
    "tl_all = vol.tokenlist(section='all')\n",
    "chapter_pages = tl_all.loc[(slice(None), slice(None), \" \"),]\n",
    "chapter_pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AAAAAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tl_simple = vol.tokenlist(pos=False, pages=False)\n",
    "# .sample(5) returns five random words from the full result\n",
    "\n",
    "tl_simple['count'] > 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relative frequencies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still looking at one volume, let's start to explore the relative frequencies of tokens within the volume. \n",
    "\n",
    "The following cell will display the most common tokens (words or punctuation marks) in a given volume, alongside the number of times they appear. It will also calculate their relative frequencies (found by dividing the number of appearances over the total number of words in the book) and display the results in a DataFrame. The cell may take a few seonds to run because we're looping through every word in the volume!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = vol.tokenlist(pos=False, case=False, pages=False).sort_values('count', ascending=False)\n",
    "\n",
    "freqs = []\n",
    "for count in tokens['count']:\n",
    "    freqs.append(count/sum(tokens['count']))\n",
    "    \n",
    "tokens['rel_frequency'] = freqs\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the most common tokens from the volume and their frequencies. The following cell outputs a bar plot using the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "# Build a list of frequencies and a list of tokens.\n",
    "freqs_1, tokens_1 = [], []\n",
    "for i in range(15):  # top 8 words\n",
    "    freqs_1.append(freqs[i])\n",
    "    tokens_1.append(tokens.index.get_level_values('lowercase')[i])\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = numpy.arange(len(tokens_1))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_1)\n",
    "plt.xticks(x_ticks, tokens_1, rotation=90)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:14] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the most common tokens are mostly punctuation and basic words that don't provide context. Let's see if we can narrow our search to gain some more relevant insight. We can get a list of stopwords from the nltk library. Punctuation is in the string library.\n",
    "\n",
    "Next we'll import nltk and make the stopwords and punctuation accessible to us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "print(stopwords.words('english'))\n",
    "\n",
    "print()\n",
    "\n",
    "print(punctuation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a list of words to ignore in our DataFrame, we can make a few tweaks to our plotting cell to remove the punctuation and display only those words not in our stopword list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs_filtered, tokens_filtered, i = [], [], 0\n",
    "while len(tokens_filtered) < 10:\n",
    "    if tokens.index.get_level_values('lowercase')[i] not in stopwords.words('english') + list(punctuation):\n",
    "        freqs_filtered.append(freqs[i])\n",
    "        tokens_filtered.append(tokens.index.get_level_values('lowercase')[i])\n",
    "    i += 1\n",
    "\n",
    "# Create a range for the x-axis\n",
    "x_ticks = numpy.arange(len(freqs_filtered))\n",
    "\n",
    "# Plot!\n",
    "plt.bar(x_ticks, freqs_filtered)\n",
    "plt.xticks(x_ticks, tokens_filtered, rotation=90)\n",
    "plt.ylabel('Frequency', fontsize=14)\n",
    "plt.xlabel('Token', fontsize=14)\n",
    "plt.title('Common token frequencies in \"' + vol.title[:14] + '...\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokens from all volumes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see how word frequencies compare across all the books in our samples. \n",
    "\n",
    "First we'll set-up a few functions. The first finds the most common noun in a volume, with adjustable parameters for minimum length. The second calculates the relative frequency of a token across the entirety of a volume, saving us the time of doing the calculation like in the above cell. Finally, we'll have a visualization function to create a bar plot of relative frequencies for all volumes in our sample, so that we can easily track how word frequencies differ across titles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1\n",
    "\n",
    "Let's see what the most common nouns in this work are by word length. To try, add a number to code below.\n",
    "\n",
    "NOTE: word_length defaults to 2. e.g. most_common_noun(fr_novels.first) returns 'time'.\n",
    "\n",
    "\n",
    "Try adding a word in the single quotes in the last line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_common_noun(vol, word_length=2):   \n",
    "    # Build a table of common nouns\n",
    "    tokens_1 = vol.tokenlist(pages=False, case=False)\n",
    "    nouns_only = tokens_1.loc[(slice(None), slice(None), ['NN']),]\n",
    "    top_nouns = nouns_only.sort_values('count', ascending=False)\n",
    "\n",
    "    token_index = top_nouns.index.get_level_values('lowercase')\n",
    "    \n",
    "    # Choose the first token at least as long as word_length with non-alphabetical characters\n",
    "    for i in range(max(token_index.shape)):\n",
    "        if (len(token_index[i]) >= word_length):\n",
    "            if(\"'\", \"!\", \",\", \"?\" not in token_index[i]):\n",
    "                return token_index[i]\n",
    "    print('There is no noun of this length')\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a number to the parenthesis after vol,\n",
    "most_common_noun(vol, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2\n",
    "Here, the function frequency() returns a plot of the usage frequencies of the given word across all volumes in the given FeatureReader collection.\n",
    "\n",
    "NOTE: frequency() returns a dictionary entry of the form {'word': frequency}. e.g. frequency(fr_novels.first(), 'blue') returns {'blue': 0.00012}\n",
    "\n",
    "Try adding a word in the single quotes in the last line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency(vol, word):\n",
    "    t1 = vol.tokenlist(pages=False, pos=False, case=False)\n",
    "    token_index = t1[t1.index.get_level_values(\"lowercase\") == word]\n",
    "    \n",
    "    if len(token_index['count'])==0:\n",
    "        return {word: 0}\n",
    "    \n",
    "    count = token_index['count'][0]\n",
    "    freq = count/sum(t1['count'])\n",
    "    \n",
    "    return {word: float('%.5f' % freq)}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a word in the quotes below\n",
    "frequency(vol, '')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Putting them together\n",
    "\n",
    "The code below returns a plot of the usage frequencies of the given word across all volumes in the given FeatureReader collection.\n",
    "\n",
    "Try adding different words to see their relative frequency in our sample.\n",
    "\n",
    "NOTE: frequencies are given as percentages rather than true ratios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_bar_plot(word, fr):\n",
    "    freqs, titles = [], []\n",
    "    for vol in fr:\n",
    "        title = vol.title\n",
    "        short_title = title[:6] + (title[6:] and '..')\n",
    "        freqs.append(100*frequency(vol, word)[word])\n",
    "        titles.append(short_title)\n",
    "        \n",
    "    # Format and plot the data\n",
    "    x_ticks = numpy.arange(len(titles))\n",
    "    plt.bar(x_ticks, freqs)\n",
    "    plt.xticks(x_ticks, titles, fontsize=10, rotation=45)\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.title('Frequency of \"' + word + '\"', fontsize=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#add a word to between the quotes\n",
    "frequency_bar_plot('', fr_novels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, that's interesting, but since all our titles are the same, it's hard to make sense of the results. Let's try plotting relative frequency over time.\n",
    "\n",
    "The code below returns a DataFrame of relative frequencies, volume years, and page counts, along with a scatter plot.\n",
    "\n",
    "NOTE: frequencies are given in percentages rather than true ratios.\n",
    "\n",
    "Try adding a word in the single quotes in the last line below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_by_year(query_word, fr):\n",
    "    volumes = pandas.DataFrame()\n",
    "    years, page_counts, query_freqs = [], [], []\n",
    "\n",
    "    for vol in fr:\n",
    "        years.append(int(vol.year))\n",
    "        page_counts.append(int(vol.page_count))\n",
    "        query_freqs.append(100*frequency(vol, query_word)[query_word])\n",
    "    \n",
    "    volumes['year'], volumes['pages'], volumes['freq'] = years, page_counts, query_freqs\n",
    "    volumes = volumes.sort_values('year')\n",
    "    \n",
    "    # Set plot dimensions and labels\n",
    "    scatter_plot = volumes.plot.scatter('year', 'freq', color='black', s=50, fontsize=12)\n",
    "    plt.ylim(0-numpy.mean(query_freqs), max(query_freqs)+numpy.mean(query_freqs))\n",
    "    plt.ylabel('Frequency (%)', fontsize=12)\n",
    "    plt.xlabel('Year', fontsize=12)\n",
    "    plt.title('Frequency of \"' + query_word + '\"', fontsize=14)\n",
    "    \n",
    "    return volumes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_by_year('nuclear', fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making use of the structured file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One particularly useful thing about the Extracted Features dataset is that the tokens in the extracted features files are part-of-speech tagged to differentiate homynyms like rose, which can be a name, a noun, and a verb.\n",
    "\n",
    "For each page, the data is also divided into a header, body, and footer section so that you can systematically remove headers or footers from your data if you choose."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We already saw the possibiity of drilling down to the part-of-speech tag earlier when we found the most frequently-occuring noun in a volume. Below, we will look for one part of speech in just the body of our volumes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = pandas.IndexSlice\n",
    "vol = next(fr.volumes())\n",
    "tl = vol.tokenlist(pages=False)\n",
    "tl.index = tl.index.droplevel(0)\n",
    "adjectives = tl.loc[idx[:,('JJ')],]\n",
    "adj_dfs = [adjectives for vol in fr.volumes()]\n",
    "all_adj = pandas.concat(adj_dfs).groupby(level='token').sum().sort_values('count', ascending=False)[:50]\n",
    "\n",
    "#prints the Pandas dataframe of all adjectives.\n",
    "print(all_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
